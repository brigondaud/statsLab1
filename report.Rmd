---
title: "Report - First practical"
author: "Jose Munoz Angulo, Vivien Marcault, Baptiste Rigondaud"
date: "February 12, 2018"
output: html_document
---

## Exercise 1

a) The simulation of 6,000 independent random vectors in dimension 201 with independent components, and put in a data frame is done
like this:

```{r eval=FALSE}
set.seed(0)
i <- 0
data <- c()
while(i < 201) {
  data <- c(data, rnorm(6000, mean = 0, sd = 1))
  i <- i + 1
}
m <- matrix(data = data, byrow = FALSE, nrow = 6000)
df <- data.frame(m)
```

b) The linear model is defined like this, $\forall{i} \in \left\{ 1, ..., 6000\right\}$:

$$
    \begin{align}
      Y_i &= X_{i, 201} \\
      X_i &= (X_{i, 1}, ..., X_{i, 200}) \\
      \epsilon_{i} &\sim \mathcal{N}(0,\sigma^{2})
    \end{align}
$$

Thus we have the equation:

$$
Y = X \beta + \epsilon
$$

The true model would be defined like this, $\forall{i} \in \left\{ 1, ..., 6000\right\}$:

$$
\beta_{i} = 0,\ \sigma^{2} = 1
$$
Because every components are simulated so that they are independent.

c) To estimate the linear model, we use this R code:

```{r eval=FALSE}
model <- lm(formula = X1 ~ ., data = df)
```
In order to compute the number of coefficients assessed as significantly non-zero at level 5%, the following code is used:


<!-- Used to eval but not show -->
```{r echo=FALSE}
set.seed(0)
i <- 0
data <- c()
while(i < 201) {
  data <- c(data, rnorm(6000, mean = 0, sd = 1))
  i <- i + 1
}
m <- matrix(data = data, byrow = FALSE, nrow = 6000)
df <- data.frame(m)

model <- lm(formula = X1 ~ ., data = df)
```

```{r}
coef_count <- sum((summary(model)$coefficients)[,"Pr(>|t|)"] <= 0.05)
print(coef_count)
```
This result is actually unexpected, in fact the number of coefficient assessed as significantly non-zero should be near zero, because there is no correlation between the predictors (they are all independent).

This is a problem because it means that there are more cases where the null hypothesis is wrongly rejected. This problem is observed when multiple hypotheses are tested, because the chance of a rare event to occur increases, thus the likelihood of rejecting the null hypothesis increases (incorrectly).

This problem can be solved by using the Bonferroni corrrection, which consists of using a signigicance level of $ \frac{\alpha}{m} $, where $m$ is the number of hypotheses.

Thus we have a new number of coefficient assessed as significantly non-zero:
```{r}
coef_count <- sum((summary(model)$coefficients)[,"Pr(>|t|)"] <= 0.05/200)
print(coef_count)
```

## Exercise 2

a) The simulation of a sample of $n=1000$ of the following model is made like so:

```{r eval=FALSE}
set.seed(3)
n <-1000
x1 <- rnorm(n = n)
x2 <- 3 * x1 + rnorm(n = n)
y = 2 + x1 + x2 + rnorm(n = n)

# Create data frame
data = c(y, x1, x2)
m = matrix(data, byrow = FALSE, nrow = n)
colnames(m) <- c("Y", "X1", "X2")
df = data.frame(m)
```

```{r a, echo=FALSE}
set.seed(3)
n <-1000
x1 <- rnorm(n = n)
x2 <- 3 * x1 + rnorm(n = n)
y = 2 + x1 + x2 + rnorm(n = n)

# Create data frame
data = c(y, x1, x2)
m = matrix(data, byrow = FALSE, nrow = n)
colnames(m) <- c("Y", "X1", "X2")
df = data.frame(m)
plot(x1, x2, xlab="X1", ylab="X2")
```

The shape of the plot seems to be linear, it can be explained by the fact that $X_2$ has an expresion that is linear with $X_1$ (with an error $\epsilon$).

b) We can estimate both models by doing:

```{r, eval=FALSE}
y1 <- lm(Y ~ X1, data = df)
y2 <- lm(Y ~ X2, data = df) 
```

The results are:

```{r, echo=FALSE}
set.seed(3)
n <-10
x1 <- rnorm(n = n)
x2 <- 3 * x1 + rnorm(n = n)
y = 2 + x1 + x2 + rnorm(n = n)

# Create data frame
data = c(y, x1, x2)
m = matrix(data, byrow = FALSE, nrow = n)
colnames(m) <- c("Y", "X1", "X2")
df = data.frame(m)

# Question 2b
y1 <- lm(Y ~ X1, data = df)
summary(y1)
# y1 :  Good p-value, X1 is significative
y2 <- lm(Y ~ X2, data = df)
summary(y2)
# y1 :  Good p-value, X2 is significative
```

In both cases, the p-value is good, meaning it is near zero. It means than in both cases, the predictors are significative in each model.

c) We estimate the model using the next piece of code:
```{r, eval=FALSE}
model <- lm(Y ~ 0 + X1 + X2, data = df, offset = rep(2, n))
```

The results in this case are:
```{r, echo=FALSE}
set.seed(3)
n <-10
x1 <- rnorm(n = n)
x2 <- 3 * x1 + rnorm(n = n)
y = 2 + x1 + x2 + rnorm(n = n)


# Create data frame
data = c(y, x1, x2)
m = matrix(data, byrow = FALSE, nrow = n)
colnames(m) <- c("Y", "X1", "X2")
df = data.frame(m)

# Create model
model <- lm(Y ~ X1 + X2, data = df, offset = rep(2, n))
summary(model)
```

In this case using both variables to estimate the model causes the model itself to have a good p-value, meaning that the such model is a well-suited estimator. However, looking at the p-value for each individual predictor we can notice that they are not really significative. This is not at all inconsistent as it is possible for the model to perfom well even under these conditions.

This can be explained by the fact that it exists a correlation between the variables X1 and X2, which can be observed in the scatter plot, causing the importance of each predictor to be less significative.

```{r, echo=FALSE}
source("./ellipse.R")
set.seed(3)
B1 <- 1
B2 <- 1
for (i in 1:100) {
n <-10
x1 <- rnorm(n = n)
x2 <- 3 * x1 + rnorm(n = n)
y = 2 + x1 + x2 + rnorm(n = n)


# Create data frame
data = c(y, x1, x2)
m = matrix(data, byrow = FALSE, nrow = n)
colnames(m) <- c("Y", "X1", "X2")
df = data.frame(m)
model <- lm(Y ~ X1 + X2, data = df, offset = rep(2, n))
B1 <- c(B1, model$coefficients[2])
B2 <- c(B2, model$coefficients[3])
}

# Create model
pdf1 <- density(B1)
q1 = quantile(B1, c(0.025, 0.975))
plot(pdf1)
segments(x0=q1[1], y0=0, y1=1, col="red")
segments(x0=q1[2], y0=0, y1=1, col="red")

pdf2 <- density(B2)
q2 = quantile(B2, c(0.025, 0.975))
plot(pdf2)
segments(x0=q2[1], y0=0, y1=1, col="red")
segments(x0=q2[2], y0=0, y1=1, col="red")

corelation = cor(B1, B2)
#ellipses(corelation, level=0.5)


```