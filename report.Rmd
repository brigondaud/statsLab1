---
title: "Report - First practical"
author: "Jose Munoz Angulo, Vivien Marcault, Baptiste Rigondaud"
date: "February 12, 2018"
output: html_document
---

## Exercise 1

a) The simulation of 6,000 independent random vectors in dimension 201 with independent components, and put in a data frame is done
like this:

```{r eval=FALSE}
set.seed(0)
i <- 0
data <- c()
while(i < 201) {
  data <- c(data, rnorm(6000, mean = 0, sd = 1))
  i <- i + 1
}
m <- matrix(data = data, byrow = FALSE, nrow = 6000)
df <- data.frame(m)
```

b) The linear model is defined like this, $\forall{i} \in \left\{ 1, ..., 6000\right\}$:

$$
    \begin{align}
      Y_i &= X_{i, 201} \\
      X_i &= (X_{i, 1}, ..., X_{i, 200}) \\
      \epsilon_{i} &\sim \mathcal{N}(0,\sigma^{2})
    \end{align}
$$

Thus we have the equation:

$$
Y = X \beta + \epsilon
$$

The true model would be defined like this, $\forall{i} \in \left\{ 1, ..., 6000\right\}$:

$$
\beta_{i} = 0,\ \sigma^{2} = 1
$$
Because every components are simulated so that they are independent.

c) To estimate the linear model, we use this R code:

```{r eval=FALSE}
model <- lm(formula = X1 ~ ., data = df)
```
In order to compute the number of coefficients assessed as significantly non-zero at level 5%, the following code is used:


<!-- Used to eval but not show -->
```{r echo=FALSE}
set.seed(0)
i <- 0
data <- c()
while(i < 201) {
  data <- c(data, rnorm(6000, mean = 0, sd = 1))
  i <- i + 1
}
m <- matrix(data = data, byrow = FALSE, nrow = 6000)
df <- data.frame(m)

model <- lm(formula = X1 ~ ., data = df)
```

```{r}
coef_count <- sum((summary(model)$coefficients)[,"Pr(>|t|)"] <= 0.05)
print(coef_count)
```
This result is actually unexpected, in fact the number of coefficient assessed as significantly non-zero should be near zero, because there is no correlation between the predictors (they are all independent).

This is a problem because it means that there are more cases where the null hypothesis is wrongly rejected. This problem is observed when multiple hypotheses are tested, because the chance of a rare event to occur increases, thus the likelihood of rejecting the null hypothesis increases (incorrectly).

This problem can be solved by using the Bonferroni corrrection, which consists of using a signigicance level of $ \frac{\alpha}{m} $, where $m$ is the number of hypotheses.

Thus we have a new number of coefficient assessed as significantly non-zero:
```{r}
coef_count <- sum((summary(model)$coefficients)[,"Pr(>|t|)"] <= 0.05/200)
print(coef_count)
```

## Exercise 2

a) The simulation of a sample of $n=1000$ of the following model is made like so:

```{r eval=FALSE}
set.seed(3)
n <-1000
x1 <- rnorm(n = n)
x2 <- 3 * x1 + rnorm(n = n)
y = 2 + x1 + x2 + rnorm(n = n)

# Create data frame
data = c(y, x1, x2)
m = matrix(data, byrow = FALSE, nrow = n)
colnames(m) <- c("Y", "X1", "X2")
df = data.frame(m)
```

```{r a, echo=FALSE}
set.seed(3)
n <-1000
x1 <- rnorm(n = n)
x2 <- 3 * x1 + rnorm(n = n)
y = 2 + x1 + x2 + rnorm(n = n)

# Create data frame
data = c(y, x1, x2)
m = matrix(data, byrow = FALSE, nrow = n)
colnames(m) <- c("Y", "X1", "X2")
df = data.frame(m)
plot(x1, x2, xlab="X1", ylab="X2")
```

The shape of the plot seems to be linear, it can be explained by the fact that $X_2$ has an expresion that is linear with $X_1$ (with an error $\epsilon$).

b) We can estimate both models by doing:

```{r, eval=FALSE}
y1 <- lm(Y ~ X1, data = df)
y2 <- lm(Y ~ X2, data = df) 
```

The results are:

```{r, echo=FALSE}
set.seed(3)
n <-10
x1 <- rnorm(n = n)
x2 <- 3 * x1 + rnorm(n = n)
y = 2 + x1 + x2 + rnorm(n = n)

# Create data frame
data = c(y, x1, x2)
m = matrix(data, byrow = FALSE, nrow = n)
colnames(m) <- c("Y", "X1", "X2")
df = data.frame(m)

# Question 2b
y1 <- lm(Y ~ X1, data = df)
summary(y1)
# y1 :  Good p-value, X1 is significative
y2 <- lm(Y ~ X2, data = df)
summary(y2)
# y1 :  Good p-value, X2 is significative
```

In both cases, the p-value is good, meaning it is near zero. It means than in both cases, the predictors are significative in each model.
